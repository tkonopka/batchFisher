---
title: "Fast computation of batches of Fisher tests"
date: "`r format(Sys.time(), '%d %B %Y')`"
---


<style>
p {
  font-size: 16px;
}
h4.date {
  margin-bottom: 2em;
}
body {
 padding-bottom: 1em;
}
</style>


The Fisher exact test is a common approach to compare two sets and R provides a straightforward way to perform this test via the `fisher.test` function. However, systematic analyses that deal with many sets evaluate the `fisher.test` many times, i.e. in batch. The `batchFisher` package optimizes such batch calculations.

The package can be installed through github. Once installed, it can be loaded in the usual manner.

```{r}
library(batchFisher)
```




# Simulation

Let's make a simulation using some synthetic data. Let's imagine there are 1000 items to choose from, and we have several subsets of these items. These subsets can be of variable size, but let's make each of them small compared to the total number of items.

```{r examples, cache=TRUE}
set.seed(1234)
all_items = 1:1000
random_sets = lapply(1:1000, function(x) {
  # sampling is from the first 500 items - so random sets with a clear bias
  sort(sample(all_items[1:500], rpois(1, 20)))
})
names(random_sets) = paste0("set_", seq_along(random_sets))
```

We can preview a couple of these sets
```{r}
random_sets[1:2]
```

The analysis of this collection might involve comparing pairs of these sets - perhaps all pairs - using the Fisher test. The motivation and interpretation of these tests in a real-world application will depend on the context. Here, let's just assume that we have a certain number of tasks to perform.

```{r tasks, cache=TRUE}
num.tasks = 5000
all.tasks = data.table(a=sample(names(random_sets), num.tasks, replace=TRUE),
                       b=sample(names(random_sets), num.tasks, replace=TRUE))
some.tasks = head(all.tasks)
some.tasks
```

Each of the tasks - a row in the `all.tasks` table - involves two specified sets. The number of tasks is capped at just 5000 as that is sufficient for this simulation. The other table `some.tasks` is a convenience; we will use it to check the code below.




# Methods

## Simple approach

The most direct approach to this problem is to iterate through the table, extract the two sets relevant for each task, perform the Fisher test, and store the result. We can encode this in a function,

```{r process.simple}
process_simple = function(tasks) {
  n = nrow(tasks)
  result = rep(NA, n)
  for (i in 1:n) {
    aset = random_sets[[tasks$a[i]]]
    bset = random_sets[[tasks$b[i]]]
    v = contingency_vector(aset, bset, length(all_items))
    result[i] = fisher.test(matrix(v, nrow=2))$p.value
  }
  result
}
```

(This implementation uses a helper function, `contingency_vector`, from the `batchFisher` package. It compares two sets and produces values that can be placed into a contingency table. While this function is from the `batchFisher` package, it has a very simple implementation.)

To see how this work, we can use this function to evaluate some of the tasks.

```{r}
process_simple(some.tasks)
```


## Batch computation

The `batchFisher` package provides alternate implementations. One of these involves collecting many contingency matrices, and then computing p-values in a distinct second step. 

```{r process.batch}
process_batch = function(tasks) {
  contingency_tables = batch_contingency(tasks$a, tasks$b,
                                         universe_size=length(all_items),
                                         sets=random_sets)
  batch_fisher(contingency_tables)$p.value
}
```

We can check this works by evaluating a few of the tasks, as before.

```{r}
process_batch(some.tasks)
```

While the output is the same as before, this strategy can in-principle provide performance gains when several tasks yield the same contingency matrix. The fisher p-value for that matrix can be computed only once and assigned to all the relevant tasks. 


## Batch computation with precomputation

In some real-world applications, sets are rather small and several sets may have the same size, so we can precompute results for some contingency tables that are bound to appear quite often. Here, let's compute results for tables that can appear for sets of sizes up to 40. 

```{r precomp.40, cache=TRUE}
precomp.40 = precompute_fisher(length(all_items), 0:40, max_or=30)
head(precomp.40, 3)
```

The table contains configurations with the first three entries are smaller or equal to 40. We can now define a new procedure to perform our tasks by using this resource.

```{r process.precomputed}
process_batch_precomp = function(tasks, precomputed) {
  contingency_tables = batch_contingency(tasks$a, tasks$b,
                                         universe_size=length(all_items),
                                         sets=random_sets)
 batch_fisher(contingency_tables, precomputed=precomputed)$p.value
}
```

We can check this generates the same output as before.

```{r}
process_batch_precomp(some.tasks, precomp.40)
```

In terms of implementation, this is almost the same as `process_batch`, but there is an additional argument. The advantage of this approach is that the precomputed values can be re-used many times. This is particularly useful when calculations consist many similar batches.



# Performance

We can now measure performance of the three implementations. When there is only a single task to perform, it should be obvious that the best approach will be to use a single `fisher.test` just like in the first approach above; any additional processing is superfluous. But when there are many similar tasks, the other approaches may provide benefits. Let's measure the running time as a function of the number of tasks.

```{r performance, cache=TRUE}
perf = data.table(n=c(1, seq(100, num.tasks, by=100)),
                  simple=0, batch=0, precomp=0)
elapsed = function(...) { system.time(...)["elapsed"] }
for (i in seq_along(perf$n)) {
  i.tasks = all.tasks[1:perf$n[i]]
  perf$simple[i]  = elapsed(process_simple(i.tasks))
  perf$batch[i]   = elapsed(process_batch(i.tasks))
  perf$precomp[i] = elapsed(process_batch_precomp(i.tasks, precomp.40))
}
```

```{r performance.plot, echo=FALSE, eval=TRUE, fig.width=10, fig.height=3.6, dpi=300}
library(Rcssplot)
RcssOverload()
RcssDefaultStyle = Rcss("batchFisher.Rcss")
implementations = c("simple", "batch", "precomp")
implementation.labels = c(simple="simple", batch="batch",
                          precomp="batch with precomp.")
colors = c()
for (i in implementations) {
 colors[i] = RcssValue("colors", i)
}
#
# panel with total running time
par(mfrow=c(1,2))
plot(c(0, num.tasks), c(0, max(perf$simple)),
       xlab="Batch size", ylab="Running time (seconds)")
axis(1)
axis(2)
for (i in implementations) {
  lines(perf$n, perf[[i]], col=colors[i], Rcssclass=i)
}
legend("topleft", implementation.labels[implementations], col=colors[implementations])
#
# panel with operations per second
ops = perf
for (i in implementations) {
  ops[[i]] = (perf$n / perf[[i]])/1000
}
plot(c(0, num.tasks), c(0, max(ops$precomp)),
       xlab="Batch size", ylab="Rate ('000s tests per second)")
axis(1)
axis(2)
for (i in implementations) {
  lines(ops$n, ops[[i]], col=colors[i], Rcssclass=i)
}
legend("right", implementation.labels[implementations], col=colors[implementations])
```

Apart from glitches, all approaches show running time that increases with the number of tests in the batch. But the slopes of trend lines differ substantially. For large batch sizes in this simulation, the speedup of the implementation with precomputation over the simple implementation is `r round(perf[n==5000]$simple/perf[n==5000]$precomp, 1)`. 

The second diagram shows operations per second. This calculation rate is constant for the simple approach, but increases for the other methods with the batch size. This is because as more tests are performed, there is more chance of seeing duplicate configurations. The best rate achieved in this simulation is ~`r signif(max(ops$precomp), 2)` thousand tests per second.

(The graphical summary hides the fact that the approach involving prcomputation requires performing work before the performance-measuring loop. That preparation time can be substantial, but the expenditure can be worthwhile when there are many tasks to process and the precomputed values can be re-used many times.)


# Discussion

The `batchFisher` package provides convenience functions for evaluating batches of Fisher tests. In certain situations of practical interest, using these convenience functions can offer substantial speedup over performing tests one-at-a-time.

All functions in the package are thread-safe; they can be used inside constructs such as `mclapply`. 





# Appendix

```{r}
sessionInfo()
```

&nbsp;